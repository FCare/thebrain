# Configuration pour llama.cpp avec Qwen3-VL GGUF
# Copiez ce fichier vers .env et modifiez selon vos besoins

# =============================================================================
# CONFIGURATION DOCKER
# =============================================================================

# Port exposé sur l'hôte
HOST_PORT="8000"

# =============================================================================
# CONFIGURATION GPU
# =============================================================================

# GPU à utiliser (all, 0, 1, etc.)
CUDA_VISIBLE_DEVICES="0"

# =============================================================================
# EXEMPLES DE CONFIGURATION
# =============================================================================

# DÉVELOPPEMENT:
# HOST_PORT="8000"
# CUDA_VISIBLE_DEVICES="0"

# PRODUCTION:
# HOST_PORT="80"
# CUDA_VISIBLE_DEVICES="0,1"

# MULTI-GPU:
# CUDA_VISIBLE_DEVICES="0,1"