# Qwen3-VL avec llama.cpp

Serveur d'API compatible OpenAI utilisant llama.cpp avec le modÃ¨le Qwen3-VL quantifiÃ© GGUF.

## ğŸš€ FonctionnalitÃ©s

- âœ… **llama.cpp natif** - Performance optimisÃ©e avec CUDA
- âœ… **Interface OpenAI Compatible** - Endpoints `/v1/chat/completions`, `/v1/models`
- âœ… **Qwen3-VL GGUF** - ModÃ¨le vision-language quantifiÃ© Q4_K_XL (~5GB)
- âœ… **Multi-Modal** - Support texte + images avec 49K context
- âœ… **Streaming** - RÃ©ponses en temps rÃ©el
- âœ… **GPU Accelerated** - Support CUDA complet
- âœ… **Docker Ready** - DÃ©ploiement simplifiÃ©

## ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Docker        â”‚â”€â”€â”€â–¶â”‚  llama.cpp       â”‚â”€â”€â”€â–¶â”‚  OpenAI API     â”‚
â”‚   (CUDA)        â”‚    â”‚  llama-server    â”‚    â”‚  Compatible     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â”‚                        â”‚                        â”‚
         â–¼                        â–¼                        â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   NVIDIA        â”‚    â”‚  Qwen3-VL-8B     â”‚    â”‚  /health        â”‚
â”‚   Runtime       â”‚    â”‚  GGUF Q4_K_XL    â”‚    â”‚  /v1/models     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“¦ Installation

### PrÃ©requis

- **GPU NVIDIA** avec 6+ GB VRAM
- **Docker + NVIDIA Container Runtime**
- **CUDA 12.8** ou compatible

### DÃ©marrage avec Docker

```bash
# Cloner le repository
git clone <repository-url>
cd qwen-llama-cpp

# Configuration (optionnel)
cp .env.example .env
# Ã‰diter .env si nÃ©cessaire

# Lancement
docker-compose up -d

# VÃ©rification
curl http://localhost:8000/health
```

### Configuration GPU

```bash
# VÃ©rifier le support NVIDIA
docker run --rm --runtime=nvidia nvidia/cuda:12.8-base nvidia-smi

# Changer de GPU (dans .env)
echo "CUDA_VISIBLE_DEVICES=1" >> .env
```

## ğŸ”§ Configuration

### Variables d'Environnement

| Variable | DÃ©faut | Description |
|----------|--------|-------------|
| `HOST_PORT` | `8000` | Port exposÃ© sur l'hÃ´te |
| `CUDA_VISIBLE_DEVICES` | `0` | GPU Ã  utiliser |

### ParamÃ¨tres llama.cpp

Le serveur est configurÃ© dans [`start.sh`](start.sh) avec :

```bash
./llama.cpp/llama-server \
    -hf unsloth/Qwen3-VL-8B-Instruct-GGUF:UD-Q4_K_XL \
    --n-gpu-layers 99 \
    --host 0.0.0.0 \
    --port 8000 \
    --ctx-size 49152 \
    --parallel 2 \
    --flash-attn on
```

## ğŸ“š Utilisation

### Chat Completion Basique

```python
import openai

client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="dummy-key"  # Non utilisÃ©
)

response = client.chat.completions.create(
    model="unsloth/Qwen3-VL-8B-Instruct-GGUF",
    messages=[
        {"role": "user", "content": "Bonjour! Comment allez-vous?"}
    ],
    max_tokens=1000,
    temperature=0.7
)

print(response.choices[0].message.content)
```

### Chat avec Images

```python
response = client.chat.completions.create(
    model="unsloth/Qwen3-VL-8B-Instruct-GGUF",
    messages=[
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Que voyez-vous dans cette image?"},
                {"type": "image_url", "image_url": {"url": "https://example.com/image.jpg"}}
            ]
        }
    ]
)
```

### Streaming

```python
stream = client.chat.completions.create(
    model="unsloth/Qwen3-VL-8B-Instruct-GGUF",
    messages=[{"role": "user", "content": "Racontez-moi une histoire"}],
    stream=True
)

for chunk in stream:
    if chunk.choices[0].delta.content:
        print(chunk.choices[0].delta.content, end="")
```

### cURL Direct

```bash
# Chat completion
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{
    "model": "unsloth/Qwen3-VL-8B-Instruct-GGUF",
    "messages": [{"role": "user", "content": "Hello"}],
    "max_tokens": 100
  }'

# Liste des modÃ¨les
curl http://localhost:8000/v1/models

# SantÃ© du service
curl http://localhost:8000/health
```

## ğŸ” API Endpoints

### OpenAI Compatible
- `POST /v1/chat/completions` - Chat completions avec support vision
- `GET /v1/models` - Liste des modÃ¨les disponibles

### llama.cpp Natif
- `POST /completion` - Completion de texte simple
- `GET /health` - Ã‰tat de santÃ© du service
- `POST /tokenize` - Tokenisation de texte
- `POST /detokenize` - DÃ©tokenisation

## âš¡ Performance

### SpÃ©cifications TestÃ©es

| GPU | VRAM | ModÃ¨le | Quantization | Performance |
|-----|------|--------|--------------|-------------|
| RTX 4090 | 24GB | Qwen3-VL-8B | Q4_K_XL | ~25 tokens/s |
| RTX 4080 | 16GB | Qwen3-VL-8B | Q4_K_XL | ~20 tokens/s |
| RTX 4070 | 12GB | Qwen3-VL-8B | Q4_K_XL | ~15 tokens/s |
| RTX 3080 | 10GB | Qwen3-VL-8B | Q4_K_XL | ~12 tokens/s |

### Optimisations GGUF

- **MÃ©moire**: ~5GB VRAM (vs ~15GB FP16)
- **Vitesse**: Performance native C++
- **Context**: Support jusqu'Ã  49K tokens

## ğŸ› ï¸ DÃ©veloppement

### Structure du Projet

```
qwen-llama-cpp/
â”œâ”€â”€ Dockerfile               # Image Docker avec llama.cpp + CUDA
â”œâ”€â”€ start.sh                 # Script de dÃ©marrage llama-server
â”œâ”€â”€ docker-compose.yml       # Orchestration
â”œâ”€â”€ .env.example            # Variables d'environnement
â””â”€â”€ README.md               # Cette documentation
```

### Build Local

```bash
# Build de l'image
docker build -t llama-qwen:latest .

# Test local
docker run --rm --runtime=nvidia \
  -p 8000:8000 \
  llama-qwen:latest
```

### Monitoring

```bash
# Logs en temps rÃ©el
docker logs -f llama-qwen-server

# MÃ©triques GPU
watch -n 1 nvidia-smi

# Ã‰tat de santÃ©
curl http://localhost:8000/health
```

## ğŸ”§ Troubleshooting

### ProblÃ¨mes Courants

**1. Erreur CUDA Out of Memory**
```bash
# Utiliser un GPU avec plus de VRAM
export CUDA_VISIBLE_DEVICES=1

# Ou rÃ©duire le contexte dans start.sh
--ctx-size 32768
```

**2. ModÃ¨le ne se tÃ©lÃ©charge pas**
```bash
# VÃ©rifier les logs
docker logs llama-qwen-server

# VÃ©rifier l'espace disque
df -h
```

**3. Pas de GPU dÃ©tectÃ©**
```bash
# VÃ©rifier le runtime NVIDIA
docker run --rm --runtime=nvidia nvidia/cuda:12.8-base nvidia-smi

# Installer nvidia-container-toolkit
sudo apt install nvidia-container-toolkit
sudo systemctl restart docker
```

## ğŸ“– Documentation API

llama.cpp n'inclut **pas de frontend de documentation automatique** comme FastAPI (`/docs`).

### Ressources disponibles
- **Endpoints** : Testez directement avec curl/Postman
- **Documentation officielle** : [llama.cpp server README](https://github.com/ggml-org/llama.cpp/blob/master/examples/server/README.md)
- **OpenAI API Reference** : Compatible avec [OpenAI Chat API](https://platform.openai.com/docs/api-reference/chat)

## ğŸ“„ License

MIT License - voir LICENSE file

## ğŸ™ Remerciements

- [llama.cpp](https://github.com/ggerganov/llama.cpp) pour le moteur d'infÃ©rence
- [Qwen Team](https://github.com/QwenLM/Qwen2-VL) pour le modÃ¨le
- [Unsloth](https://unsloth.ai/) pour la quantification GGUF